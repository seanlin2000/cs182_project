{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "# import sys\n",
    "# sys.path.append(root_folder)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import pathlib\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "from models.convengers import *\n",
    "from models.solver import NickFury\n",
    "from utils import dictionary\n",
    "from utils import visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder = \"data/tiny-imagenet-200/\"\n",
    "data_dir = pathlib.Path(dataset_folder)\n",
    "\n",
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_datasets = {x: datasets.ImageFolder(data_dir / x, data_transforms[x]) for x in ['train', 'val']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=20, shuffle=True, pin_memory=True) for x in ['train', 'val']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chpt = torch.load(\"latest.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['net'])\n"
     ]
    }
   ],
   "source": [
    "print(chpt.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.resnet101(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 21, 159, 119, 167,  15, 193,  27, 123,   6,  85,  68, 103, 111, 137,\n",
      "         44, 159,  22,  25,  69,  65])\n",
      "tensor([267, 601, 459, 446, 348, 616, 979,  88, 220, 797, 430, 975, 852, 987,\n",
      "        323, 673, 570, 220, 721, 569])\n"
     ]
    }
   ],
   "source": [
    "inputs, classes = next(iter(dataloaders['train']))\n",
    "class_preds = model(inputs)\n",
    "print(classes)\n",
    "print(torch.argmax(class_preds,dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fgsm_attack(image, epsilon, data_grad):\n",
    "    # Collect the element-wise sign of the data gradient\n",
    "    sign_data_grad = data_grad.sign()\n",
    "    # Create the perturbed image by adjusting each pixel of the input image\n",
    "    perturbed_image = image + epsilon*sign_data_grad\n",
    "    # Adding clipping to maintain [0,1] range\n",
    "    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "    # Return the perturbed image\n",
    "    return perturbed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "adversary_classes = torch.zeros(classes.shape,dtype=torch.long)\n",
    "input_images = Variable(inputs, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scores = model(input_images)\n",
    "criterion =  nn.CrossEntropyLoss()\n",
    "loss = criterion(model_scores, adversary_classes)\n",
    "loss.backward()\n",
    "\n",
    "image_grad = input_images.grad.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n",
      "tensor([182,  24, 167, 110, 144, 119,  20, 126, 159, 116,  81,   5, 174, 146,\n",
      "         89,  95, 155, 191,  44, 161])\n",
      "\n",
      "tensor(0.1000)\n",
      "tensor([161,  37, 109,  29,  90, 133,  20,  33,  85,  89, 154,  10,   0, 146,\n",
      "        136,  95,  81, 191,  44,   0])\n",
      "\n",
      "tensor(0.2000)\n",
      "tensor([161, 143, 109,   0,  90, 147,  20, 136,  30,   4, 157,  26,  47,  89,\n",
      "        134, 173, 127, 191, 159, 199])\n",
      "\n",
      "tensor(0.3000)\n",
      "tensor([161, 143, 139,   0,  90, 142,  20, 190, 149,   5, 157, 152,   0, 146,\n",
      "        113,  95, 127, 191,   0,  14])\n",
      "\n",
      "tensor(0.4000)\n",
      "tensor([188, 143, 117, 197, 105, 133,  20, 164, 149,  89, 133,  14,   0, 110,\n",
      "         65, 139, 127, 181,  41, 185])\n",
      "\n",
      "tensor(0.5000)\n",
      "tensor([185, 143,  72,  53, 149, 133,  20, 159, 149,  80, 119, 196,   0, 146,\n",
      "        113, 139, 127, 191,  36,  25])\n",
      "\n",
      "tensor(0.6000)\n",
      "tensor([ 83, 143, 139,  22, 149, 167,  22,  59, 149, 159, 112,   4,   0, 146,\n",
      "        113,  95, 127, 191,  37, 196])\n",
      "\n",
      "tensor(0.7000)\n",
      "tensor([185, 143, 119,  55, 120, 129,  20, 199,  16,  63, 108, 157,   0, 146,\n",
      "        105,  22, 127, 191,  37,  14])\n",
      "\n",
      "tensor(0.8000)\n",
      "tensor([187, 143, 146,  22, 149, 151,  20,  77, 149,  44, 115,  33, 196, 146,\n",
      "        127, 137, 127,  82,   8,  34])\n",
      "\n",
      "tensor(0.9000)\n",
      "tensor([199, 148, 146,  27, 149, 185,  20,  82, 123,  40, 166,  55,   0,  75,\n",
      "         65,  95, 127, 153,  38,  34])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epsilon in torch.arange(0,1,0.1):\n",
    "    adversaries = fgsm_attack(input_images, epsilon, image_grad)\n",
    "    adverary_scores = model(adversaries)\n",
    "    print(epsilon)\n",
    "    print(torch.argmax(adverary_scores,dim=1))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
